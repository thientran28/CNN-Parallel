{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Parallel CNN layers for image classification\n",
        "**Note: File notebook này chỉ chứa phần cài đặt, giải thích cho thuật toán, đo thời gian chạy và tính độ chính xác. Các nội dung khác như so sánh với thuật toán tuần tự và các phiên bản sẽ được thể hiện đầy đủ trong file báo cáo**"
      ],
      "metadata": {
        "id": "4LySzmcIPMmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mục lục:\n",
        "### 1/ Bảng phân chia công việc:\n",
        "### 2/ Cài đặt các hàm song song cho các lớp:\n",
        "#### 2.1/ Các hàm song song cho lớp Convolutional:\n",
        "#### 2.2/ Các hàm song song cho lớp Max pooling:\n",
        "#### 2.3/ Các hàm song song cho lớp Softmax:\n",
        "#### 2.4/ Cài đặt các hàm song song vào trong các lớp trong mạng CNN:\n",
        "#### 2.5/ Cài đặt các hàm để huấn luyện cho mạng CNN:\n",
        "### 3/ Đo thời gian train với tập dữ liệu 10000 ảnh chữ viết tay từ tensorflow:\n",
        "### 4/ Tính độ chính xác của mạng CNN khi dự đoán cho tập test 10000 ảnh khác với tập train do tensorflow cung cấp:"
      ],
      "metadata": {
        "id": "580NBej60OZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1/ Bảng phân chia công việc"
      ],
      "metadata": {
        "id": "jMPsri9uQdDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Thành viên | MSSV |\n",
        "|--------|------------|\n",
        "| Nguyễn Thế Hưng | 19127154 |\n",
        "| Trần Minh Thiện | 19127281 |\n",
        "| Lê Tâm Anh | 19127330 |"
      ],
      "metadata": {
        "id": "pSlR_LyOPhmo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2/ Cài đặt các hàm song song cho các lớp:"
      ],
      "metadata": {
        "id": "9h8pwTIoQzvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numba import config\n",
        "from numba import jit, cuda, prange\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import time"
      ],
      "metadata": {
        "id": "4wVQdkeSPNiU"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYTM62eTY1af"
      },
      "source": [
        "# 2.1/ Các hàm song song cho lớp Convolutional:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hàm cnn_forward kernel"
      ],
      "metadata": {
        "id": "luqetZui7_XY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "rooLF511owYL"
      },
      "outputs": [],
      "source": [
        "@cuda.jit\n",
        "def cnn_forward_kernel(patches, kernels, convolution_output):\n",
        "    r, c = cuda.grid(2)\n",
        "\n",
        "    if r < patches.shape[0] and c < patches.shape[1]:\n",
        "        # mỗi luồng sẽ tính toán ra một mảng 16 phần tử (do có 16 kernel cnn)\n",
        "        for k in range(kernels.shape[0]):\n",
        "            sum = 0\n",
        "            # từ đây là phần thực hiện nhân kernel với 1 phần của hình ảnh\n",
        "            for i in range(kernels.shape[1]):\n",
        "                for j in range(kernels.shape[2]):\n",
        "                    # 2 vòng lặp ở đây vì CUDA không hỗ trợ nhân 2 ma trận với nhau\n",
        "                    # nên ta nhân từng phần tử của patches và kernel rồi cộng tổng lại\n",
        "                    sum += patches[r, c, i, j] * kernels[k, i, j]\n",
        "            convolution_output[r, c, k] = sum\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Ta cài đặt hàm forward_prop của lớp CNN với ý tưởng là kết quả trả về convolution_output có kích thước 26 x 26 x 16. Thay vì loop qua 26 x 26 mảng patches, ta sẽ tính nó song song với nhau (26 x 26 luồng tính toán song song).\n",
        "- Mỗi luồng sẽ tính toán ra kết quả là một mạng 16 phần tử ứng với số kernel cnn là 16.\n",
        "- Mỗi phần tử của ma trận patches (là một ma trận (3 x 3) khác) sẽ được nhân với ma trận kernel và rồi được tính tổng của các phần của ma trận kết quả."
      ],
      "metadata": {
        "id": "t28YBQPu9Gpu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hàm cnn_backward_kernel"
      ],
      "metadata": {
        "id": "gDimTgpw8_xq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def cnn_backward_kernel(patches, dE_dY, dE_dk):\n",
        "    x, y, z = cuda.grid(3)\n",
        "\n",
        "    if x < dE_dk.shape[0] and y < dE_dk.shape[1] and z < dE_dk.shape[2]:\n",
        "        # mỗi luồng sẽ tính toán ra một phần tử của ma trận gradient\n",
        "        temp = 0\n",
        "        for h in range(patches.shape[0]):\n",
        "            for w in range(patches.shape[1]):\n",
        "                temp += patches[h, w, y, z] * dE_dY[h, w, x]\n",
        "        dE_dk[x, y, z] = temp"
      ],
      "metadata": {
        "id": "G9voZ5V98_9c"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Ma trận trả về dE_dk có kích thước (16, 3, 3), ta sẽ sử dụng CUDA grid 3 chiều để tính toán song song từng phần tử của ma trận dE_dk."
      ],
      "metadata": {
        "id": "pXqg2HCRcXTP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K05mzy-EZTZv"
      },
      "source": [
        "# 2.2/ Các hàm song song cho lớp Max Pooling:\n",
        "**Các hàm bên dưới được nghiên cứu và cài đặt song song cho lớp Maxpooling, tuy nhiên sẽ không được sử dụng cho phiên bản cuối cùng này do thời gian chạy của cả thuật toán không được cải thiện so với các phiên bản trước**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "2mLxP8vGcrJk"
      },
      "outputs": [],
      "source": [
        "#@cuda.jit(device=True)\n",
        "def patches_generator(image,kernel_size):\n",
        "        \"\"\"\n",
        "        Divide the input image in patches to be used during pooling.\n",
        "        Yields the tuples containing the patches and their coordinates.\n",
        "        \"\"\"\n",
        "        # Compute the ouput size\n",
        "        output_h = image.shape[0] // kernel_size\n",
        "        output_w = image.shape[1] // kernel_size\n",
        "        #self.image = image\n",
        "        #c,r=cuda.grid(2)\n",
        "        for h in range(output_h):\n",
        "            for w in range(output_w):\n",
        "                patch = image[(h*kernel_size):(h*kernel_size+kernel_size), (w*kernel_size):(w*kernel_size+kernel_size)]\n",
        "                yield patch,h,w\n",
        "\n",
        "def forward_prop(image,kernel_size):\n",
        "        image_h, image_w, num_kernels = image.shape\n",
        "        max_pooling_output = np.zeros((image_h//kernel_size, image_w//kernel_size, num_kernels))\n",
        "        for patch, h, w in patches_generator(image,kernel_size):\n",
        "            max_pooling_output[h,w] = np.amax(patch, axis=(0,1))\n",
        "        return max_pooling_output\n",
        "\n",
        "\n",
        "\n",
        "def back_prop(image,dE_dY,kernel_size):\n",
        "        \"\"\"\n",
        "        Takes the gradient of the loss function with respect to the output and computes the gradients of the loss function with respect\n",
        "        to the kernels' weights.\n",
        "        dE_dY comes from the following layer, typically softmax.\n",
        "        There are no weights to update, but the output is needed to update the weights of the convolutional layer.\n",
        "        \"\"\"\n",
        "        dE_dk_temp1 = np.zeros(image.shape)\n",
        "        #dE_dk=np.ascontiguousarray(dE_dk_temp1)\n",
        "        #cuda.pinned(dE_dk)\n",
        "        for patch,h,w in patches_generator(image,kernel_size):\n",
        "            image_h, image_w, num_kernels = patch.shape\n",
        "            max_val = np.amax(patch, axis=(0,1))\n",
        "            block_size = (16, 16)\n",
        "            grid_size = (math.ceil(image_w / block_size[0]),\n",
        "                        math.ceil(image_h / block_size[1]))\n",
        "            patch=np.ascontiguousarray(patch)\n",
        "\n",
        "            dA=cuda.to_device(patch)\n",
        "            dE=cuda.to_device(dE_dk_temp1)\n",
        "            back_prob_sup[grid_size, block_size](image_h,image_w,num_kernels,dA,max_val,dE_dY,dE,h,w,kernel_size)\n",
        "            dE_dk=dE.copy_to_host()\n",
        "            return dE_dk\n",
        "\n",
        "@cuda.jit\n",
        "def back_prob_sup(image_h,image_w,num_kernels,patch,max_val,dE_dY,dE_dk,h,w,kernel_size):\n",
        "    c,r=cuda.grid(2)\n",
        "    #print(\"hihi\")\n",
        "    if r < image_h and c < image_w:\n",
        "          for idx_k in range(num_kernels):\n",
        "                if patch[r,c,idx_k] == max_val[idx_k]:\n",
        "                  #print(\"hihi\")\n",
        "                  dE_dk[h*kernel_size+r, w*kernel_size+c, idx_k] = dE_dY[h,w,idx_k]\n",
        "    #return dE_dk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMRZrx-iwVvA"
      },
      "source": [
        "# 2.3/ Các hàm song song cho lớp Softmax:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hàm nhân vector với ma trận"
      ],
      "metadata": {
        "id": "mkibEGsm3RsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hàm nhân vector với ma trận\n",
        "@cuda.jit\n",
        "def dot(a, b, c):\n",
        "  #Sử dụng một grid để duyệt ma trận\n",
        "  col = cuda.grid(1)\n",
        "  if (col < b.shape[1]):\n",
        "    sum = 0.0\n",
        "    for i in range(b.shape[0]):\n",
        "      sum += a[i] * b[i, col]\n",
        "    c[col] = sum\n"
      ],
      "metadata": {
        "id": "LLtgpsslTCTo"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cài đặt hàm nhân giữa vector với ma trận với ý tưởng đơn giản là dùng một grid để duyệt ma trận và tiến hành nhân từng phần tử của vector cho phần tử tương ứng của ma trận rồi cộng chúng lại.\n",
        "- Sau đó, gọi hàm nhân vector với ma trận thay cho hàm tuần tự ở bên trong hàm forward_prop() của lớp Softmax. Đồng bộ quá trình đọc ghi bộ nhớ giữa thiết bị và host bằng cách sử dụng các hàm cuda.to_device() và copy_to_host()."
      ],
      "metadata": {
        "id": "BYxFoMZO7pkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hàm nhân ma trận với vector"
      ],
      "metadata": {
        "id": "zBL_xFpxTEPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hàm nhân ma trận với vector\n",
        "@cuda.jit\n",
        "def cu_matrix_vector(A, b, c):\n",
        "  row = cuda.grid(1)\n",
        "  if (row < A.shape[0]):\n",
        "    sum = 0.0\n",
        "    for i in range(A.shape[1]):\n",
        "      sum += A[row, i] * b[i]\n",
        "    c[row] = sum"
      ],
      "metadata": {
        "id": "RNiZHETu2YEH"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cài đặt hàm với ý tưởng tương tự như hàm Vector nhân với ma trận ở phía trên (gọi grid với giá trị là 1 và sử dụng cho ma trận)\n",
        "- Tương tự như trên, gọi hàm nhân ma trận với vector thay cho hàm tuần tự ở bên trong hàm back_prop() của lớp Softmax. Đồng bộ quá trình đọc ghi bộ nhớ giữa thiết bị và host bằng cách sử dụng các hàm cuda.to_device() và copy_to_host()."
      ],
      "metadata": {
        "id": "DZpNbyTZ78J-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hàm nhân hai ma trận"
      ],
      "metadata": {
        "id": "PPJA9vK92aXD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "LOVIFj25zufr"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Hàm nhân hai ma trận\n",
        "@cuda.jit\n",
        "def matmul(A,B,C):\n",
        "  i,j = cuda.grid(2)\n",
        "  if i < C.shape[0] and j < C.shape[1]:\n",
        "    tmp = A[i,0] * B[0,j]\n",
        "    C[i,j] = tmp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Sử dụng 2 grid để duyệt cho hàng(i) và cột(j) của ma trận kết quả, do ta đã biết trước kích thước hai ma trận lần lượt là (2701,1) và (1,10) nên ta có thể không sử dụng vòng lặp mà gán thẳng index cột của ma trận A là 0 và tương tự index hàng của ma trận B là 0.\n",
        "- Gọi hàm nhân hai trận bên trong lớp hàm back_prop() của lớp softmax để thay thế cho hàm nhân hai ma trận tuần tự ban đầu.\n",
        "\n"
      ],
      "metadata": {
        "id": "BOKMHhVO8UAU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_TlPrapxASM"
      },
      "source": [
        "# 2.4/ Cài đặt các hàm song song vào trong các lớp trong mạng CNN:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "5VK4gXE3w-Pc"
      },
      "outputs": [],
      "source": [
        "class ConvolutionLayer:\n",
        "    def __init__(self, kernel_num, kernel_size):\n",
        "        \"\"\"\n",
        "        Constructor takes as input the number of kernels and their size. I assume only squared filters of size kernel_size x kernel_size\n",
        "        \"\"\"\n",
        "        self.kernel_num = kernel_num\n",
        "        self.kernel_size = kernel_size\n",
        "        # Generate random filters of shape (kernel_num, kernel_size, kernel_size). Divide by kernel_size^2 for weight normalization\n",
        "        self.kernels = np.random.randn(kernel_num, kernel_size, kernel_size) / (kernel_size**2)\n",
        "\n",
        "    def patches_generator(self, image):\n",
        "        \"\"\"\n",
        "        Divide the input image in patches to be used during convolution.\n",
        "        Yields the tuples containing the patches and their coordinates.\n",
        "        \"\"\"\n",
        "        # Extract image height and width\n",
        "        image_h, image_w = image.shape\n",
        "        self.image = image\n",
        "        # The number of patches, given a fxf filter is h-f+1 for height and w-f+1 for width\n",
        "        patches = np.empty((image_h-self.kernel_size+1, image_w-self.kernel_size+1, self.kernel_size, self.kernel_size))\n",
        "        for h in range(image_h-self.kernel_size+1):\n",
        "            for w in range(image_w-self.kernel_size+1):\n",
        "                patches[h, w] = image[h:(h+self.kernel_size), w:(w+self.kernel_size)]\n",
        "        return patches\n",
        "\n",
        "    def forward_prop(self, image):\n",
        "        \"\"\"\n",
        "        Perform forward propagation for the convolutional layer.\n",
        "        \"\"\"\n",
        "        # Extract image height and width\n",
        "        image_h, image_w = image.shape\n",
        "        # Initialize the convolution output volume of the correct size\n",
        "        convolution_output = np.zeros((image_h-self.kernel_size+1, image_w-self.kernel_size+1, self.kernel_num))\n",
        "        # Unpack the generator\n",
        "        patches = self.patches_generator(image)\n",
        "        block_size = (16, 16)\n",
        "        grid_size = (math.ceil(convolution_output.shape[1] / block_size[0]),\n",
        "                     math.ceil(convolution_output.shape[0] / block_size[1]))\n",
        "        cnn_forward_kernel[grid_size, block_size](patches, self.kernels, convolution_output)\n",
        "        cuda.synchronize()\n",
        "        return convolution_output\n",
        "\n",
        "    def back_prop(self, dE_dY, alpha):\n",
        "        \"\"\"\n",
        "        Takes the gradient of the loss function with respect to the output and computes the gradients of the loss function with respect\n",
        "        to the kernels' weights.\n",
        "        dE_dY comes from the following layer, typically max pooling layer.\n",
        "        It updates the kernels' weights\n",
        "        \"\"\"\n",
        "        # Initialize gradient of the loss function with respect to the kernel weights\n",
        "        dE_dk = np.zeros(self.kernels.shape)\n",
        "        patches = self.patches_generator(self.image)\n",
        "        block_size = (16, 4, 4)\n",
        "        grid_size = (math.ceil(dE_dk.shape[2] / block_size[0]),\n",
        "                     math.ceil(dE_dk.shape[1] / block_size[1]),\n",
        "                     math.ceil(dE_dk.shape[0] / block_size[2]))\n",
        "        cnn_backward_kernel[grid_size, block_size](patches, dE_dY, dE_dk)\n",
        "        cuda.synchronize()\n",
        "        # Update the parameters\n",
        "        self.kernels -= alpha*dE_dk\n",
        "        return dE_dk\n",
        "\n",
        "\n",
        "class MaxPoolingLayer:\n",
        "    def __init__(self, kernel_size):\n",
        "        \"\"\"\n",
        "        Constructor takes as input the size of the kernel\n",
        "        \"\"\"\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def patches_generator(self, image):\n",
        "        \"\"\"\n",
        "        Divide the input image in patches to be used during pooling.\n",
        "        Yields the tuples containing the patches and their coordinates.\n",
        "        \"\"\"\n",
        "        # Compute the ouput size\n",
        "        output_h = image.shape[0] // self.kernel_size\n",
        "        output_w = image.shape[1] // self.kernel_size\n",
        "        self.image = image\n",
        "\n",
        "        for h in range(output_h):\n",
        "            for w in range(output_w):\n",
        "                patch = image[(h*self.kernel_size):(h*self.kernel_size+self.kernel_size), (w*self.kernel_size):(w*self.kernel_size+self.kernel_size)]\n",
        "                yield patch, h, w\n",
        "\n",
        "    def forward_prop(self, image):\n",
        "        image_h, image_w, num_kernels = image.shape\n",
        "        max_pooling_output = np.zeros((image_h//self.kernel_size, image_w//self.kernel_size, num_kernels))\n",
        "        for patch, h, w in self.patches_generator(image):\n",
        "            max_pooling_output[h,w] = np.amax(patch, axis=(0,1)) # Chia nhỏ để tìm số lớn nhất song song sau đó so sánh với nhau\n",
        "        return max_pooling_output\n",
        "\n",
        "    def back_prop(self, dE_dY):\n",
        "        \"\"\"\n",
        "        Takes the gradient of the loss function with respect to the output and computes the gradients of the loss function with respect\n",
        "        to the kernels' weights.\n",
        "        dE_dY comes from the following layer, typically softmax.\n",
        "        There are no weights to update, but the output is needed to update the weights of the convolutional layer.\n",
        "        \"\"\"\n",
        "        dE_dk = np.zeros(self.image.shape)\n",
        "        for patch,h,w in self.patches_generator(self.image):\n",
        "            image_h, image_w, num_kernels = patch.shape\n",
        "            max_val = np.amax(patch, axis=(0,1))\n",
        "\n",
        "            for idx_h in range(image_h):\n",
        "                for idx_w in range(image_w):\n",
        "                    for idx_k in range(num_kernels):\n",
        "                        if patch[idx_h,idx_w,idx_k] == max_val[idx_k]:\n",
        "                            dE_dk[h*self.kernel_size+idx_h, w*self.kernel_size+idx_w, idx_k] = dE_dY[h,w,idx_k]\n",
        "            return dE_dk\n",
        "\n",
        "\n",
        "class SoftmaxLayer:\n",
        "    \"\"\"\n",
        "    Takes the volume coming from convolutional & pooling layers. It flattens it and it uses it in the next layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_units, output_units):\n",
        "        # Initiallize weights and biases\n",
        "        self.weight = np.random.randn(input_units, output_units)/input_units\n",
        "        self.bias = np.zeros(output_units)\n",
        "\n",
        "    def forward_prop(self, image):\n",
        "      self.original_shape = image.shape # stored for backprop\n",
        "      # Flatten the image\n",
        "      #print(\"image: \", image)\n",
        "      image_flattened = image.flatten()\n",
        "      #print(\"image_flattened: \", image_flattened)\n",
        "      self.flattened_input = image_flattened # stored for backprop\n",
        "\n",
        "      # Perform matrix multiplication and add bias\n",
        "      C = np.empty(10)\n",
        "      dA = cuda.to_device(image_flattened)\n",
        "      dB = cuda.to_device(self.weight)\n",
        "      dC = cuda.to_device(C)\n",
        "      dot[(self.weight.shape[0]+255)//256, 256](dA,dB,dC)\n",
        "      #cu_matrix_vector[(dZ_dX.shape[0]+511)//512, 512](dZ_dX,dE_dZ,C)\n",
        "      result = dC.copy_to_host()\n",
        "      first_output = result  + self.bias\n",
        "      self.output = first_output\n",
        "      # Apply softmax activation\n",
        "      softmax_output = np.exp(first_output) / np.sum(np.exp(first_output), axis=0)\n",
        "\n",
        "      return softmax_output\n",
        "\n",
        "\n",
        "    def back_prop(self, dE_dY, alpha):\n",
        "      for i, gradient in enumerate(dE_dY):\n",
        "        if gradient == 0:\n",
        "          continue\n",
        "        transformation_eq = np.exp(self.output)\n",
        "        S_total = np.sum(transformation_eq)\n",
        "\n",
        "        # Compute gradients with respect to output (Z)\n",
        "        dY_dZ = -transformation_eq[i]*transformation_eq / (S_total**2)\n",
        "        dY_dZ[i] = transformation_eq[i]*(S_total - transformation_eq[i]) / (S_total**2)\n",
        "\n",
        "        # Compute gradients of output Z with respect to weight, bias, input\n",
        "        dZ_dw = self.flattened_input\n",
        "        dZ_db = 1\n",
        "        dZ_dX = self.weight\n",
        "\n",
        "        # Gradient of loss with respect ot output\n",
        "        dE_dZ = gradient * dY_dZ\n",
        "\n",
        "        # Gradient of loss with respect to weight, bias, input\n",
        "\n",
        "        #C = np.empty((dZ_dw[np.newaxis].T.shape[0], dE_dZ[np.newaxis].shape[1]))\n",
        "        #dA = cuda.to_device(dZ_dw[np.newaxis].T)\n",
        "        #dB = cuda.to_device(dE_dZ[np.newaxis])\n",
        "        #dC = cuda.to_device(C)\n",
        "        #blockx = int(np.ceil(C.shape[0] / 16))\n",
        "        #blocky = int(np.ceil(C.shape[1] / 16))\n",
        "        #blockspergrid = (blockx, blocky)\n",
        "        #matmul[blockspergrid, (16,16)](dA,dB,dC)\n",
        "        #dE_dW = dC.copy_to_host()\n",
        "        dE_dW = dZ_dw[np.newaxis].T @ dE_dZ[np.newaxis]\n",
        "\n",
        "        # Matrix-vector multiply function\n",
        "        C = np.empty(dZ_dX.shape[0])\n",
        "        dA = cuda.to_device(dZ_dX)\n",
        "        dB = cuda.to_device(dE_dZ)\n",
        "        dC = cuda.to_device(C)\n",
        "        cu_matrix_vector[(dZ_dX.shape[0]+15)//16, 16](dA,dB,dC)\n",
        "        dE_dX = dC.copy_to_host()\n",
        "\n",
        "       # dE_dX = dZ_dX @ dE_dZ\n",
        "\n",
        "        # Update parameters\n",
        "        self.weight -= alpha * dE_dW\n",
        "        self.bias -= alpha * (dE_dZ * dZ_db)\n",
        "\n",
        "        return dE_dX.reshape(self.original_shape)\n",
        "\n",
        "def CNN_forward(image, label, layers):\n",
        "    output = image/255.\n",
        "    for layer in layers:\n",
        "        output = layer.forward_prop(output)\n",
        "    # Compute loss (cross-entropy) and accuracy\n",
        "    loss = -np.log(output[label])\n",
        "    accuracy = 1 if np.argmax(output) == label else 0\n",
        "    return output, loss, accuracy\n",
        "\n",
        "def CNN_backprop(gradient, layers, alpha=0.05):\n",
        "    grad_back = gradient\n",
        "    for layer in layers[::-1]:\n",
        "        if type(layer) in [ConvolutionLayer, SoftmaxLayer]:\n",
        "            grad_back = layer.back_prop(grad_back, alpha)\n",
        "        elif type(layer) == MaxPoolingLayer:\n",
        "            grad_back = layer.back_prop(grad_back)\n",
        "    return grad_back\n",
        "\n",
        "\n",
        "def CNN_training(image, label, layers, alpha=0.05):\n",
        "    # Forward step\n",
        "    output, loss, accuracy = CNN_forward(image, label, layers)\n",
        "\n",
        "    # Initial gradient\n",
        "    gradient = np.zeros(10)\n",
        "    gradient[label] = -1/output[label]\n",
        "\n",
        "    # Backprop step\n",
        "    gradient_back = CNN_backprop(gradient, layers, alpha)\n",
        "\n",
        "    return loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWYqMGGUZjGg"
      },
      "source": [
        "# 2.5/ Cài đặt các hàm để huấn luyện cho mạng CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YKXC0Rljm2j"
      },
      "source": [
        "## Train function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "5_mA6IhAQrEF"
      },
      "outputs": [],
      "source": [
        "def train(network, X_train, y_train, epochs = 1, learning_rate = 0.05, verbose = True):\n",
        "    for epoch in range(1):\n",
        "        if verbose == True:\n",
        "            print('Epoch {} ->'.format(epoch+1))\n",
        "        # Shuffle training data\n",
        "        permutation = np.random.permutation(len(X_train))\n",
        "        X_train = X_train[permutation]\n",
        "        y_train = y_train[permutation]\n",
        "        # Training the CNN\n",
        "        loss = 0\n",
        "        accuracy = 0\n",
        "        for i, (image, label) in enumerate(zip(X_train, y_train)):\n",
        "            if i % 100 == 0: # Every 100 examples\n",
        "                if verbose == True:\n",
        "                    print(\"Step {}. For the last 100 steps: average loss {}, accuracy {}\".format(i+1, loss/100, accuracy))\n",
        "                loss = 0\n",
        "                accuracy = 0\n",
        "            loss_1, accuracy_1 = CNN_training(image, label, network)\n",
        "            loss += loss_1\n",
        "            accuracy += accuracy_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa_FDwW3jslW"
      },
      "source": [
        "## Predict & Evaluate functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Lta9bV4iQx32"
      },
      "outputs": [],
      "source": [
        "def predict(network, image):\n",
        "    output = image/255.\n",
        "    for layer in network:\n",
        "        output = layer.forward_prop(output)\n",
        "    return np.argmax(output) # return a number\n",
        "def evaluate(network, X_test, y_test):\n",
        "    correct = 0\n",
        "    for x, y in zip(X_test, y_test):\n",
        "        pred = predict(network, x)\n",
        "        if y == pred:\n",
        "            correct += 1\n",
        "    acc = correct / y_test.shape[0]\n",
        "    print(f'Accuracy for the test set is {acc *100}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOOaJp_Oj3wa"
      },
      "source": [
        "## Loading train data and define network layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "0OaYdNsHWnjZ"
      },
      "outputs": [],
      "source": [
        "network = [\n",
        "    ConvolutionLayer(16,3), # layer with 8 3x3 filters, output (26,26,16)\n",
        "    MaxPoolingLayer(2), # pooling layer 2x2, output (13,13,16)\n",
        "    SoftmaxLayer(13*13*16, 10) # softmax layer with 13*13*16 input and 10 output\n",
        "    ]\n",
        "\n",
        "# Load training data\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train[:10000]\n",
        "y_train = y_train[:10000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wolseb1OkAqH"
      },
      "source": [
        "# 3/ Đo thời gian train với tập dữ liệu 10000 ảnh chữ viết tay từ tensorflow:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N8p_68OQ-CX",
        "outputId": "2557ad57-5431-4b9f-c4e0-8538c6df38d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 ->\n",
            "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 4 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:885: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 11 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 4 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:885: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 101. For the last 100 steps: average loss 1.8398776354200168, accuracy 35\n",
            "Step 201. For the last 100 steps: average loss 1.0733178308394276, accuracy 61\n",
            "Step 301. For the last 100 steps: average loss 0.8853509278004641, accuracy 74\n",
            "Step 401. For the last 100 steps: average loss 0.7415628064241059, accuracy 78\n",
            "Step 501. For the last 100 steps: average loss 0.6056768184953838, accuracy 84\n",
            "Step 601. For the last 100 steps: average loss 0.7379358482179693, accuracy 77\n",
            "Step 701. For the last 100 steps: average loss 0.7448088161831349, accuracy 76\n",
            "Step 801. For the last 100 steps: average loss 0.6712688302644545, accuracy 77\n",
            "Step 901. For the last 100 steps: average loss 0.5873551068948185, accuracy 84\n",
            "Step 1001. For the last 100 steps: average loss 0.6794972220205702, accuracy 80\n",
            "Step 1101. For the last 100 steps: average loss 0.6595631861478922, accuracy 79\n",
            "Step 1201. For the last 100 steps: average loss 0.5299527740634096, accuracy 81\n",
            "Step 1301. For the last 100 steps: average loss 0.5113705381011028, accuracy 87\n",
            "Step 1401. For the last 100 steps: average loss 0.6289472521216194, accuracy 83\n",
            "Step 1501. For the last 100 steps: average loss 0.40055611821311277, accuracy 87\n",
            "Step 1601. For the last 100 steps: average loss 0.49818572674555994, accuracy 84\n",
            "Step 1701. For the last 100 steps: average loss 0.3570119457143797, accuracy 89\n",
            "Step 1801. For the last 100 steps: average loss 0.2784516339534304, accuracy 94\n",
            "Step 1901. For the last 100 steps: average loss 0.3594662251991884, accuracy 90\n",
            "Step 2001. For the last 100 steps: average loss 0.40962888293809896, accuracy 84\n",
            "Step 2101. For the last 100 steps: average loss 0.41735374143355713, accuracy 88\n",
            "Step 2201. For the last 100 steps: average loss 0.3605800645779519, accuracy 88\n",
            "Step 2301. For the last 100 steps: average loss 0.4836522625460191, accuracy 84\n",
            "Step 2401. For the last 100 steps: average loss 0.34250095232344874, accuracy 91\n",
            "Step 2501. For the last 100 steps: average loss 0.5376297133634433, accuracy 85\n",
            "Step 2601. For the last 100 steps: average loss 0.41243677085961444, accuracy 86\n",
            "Step 2701. For the last 100 steps: average loss 0.4931418964579654, accuracy 85\n",
            "Step 2801. For the last 100 steps: average loss 0.21611181949146338, accuracy 92\n",
            "Step 2901. For the last 100 steps: average loss 0.5097141926751765, accuracy 84\n",
            "Step 3001. For the last 100 steps: average loss 0.39548045914902624, accuracy 91\n",
            "Step 3101. For the last 100 steps: average loss 0.3507758057844484, accuracy 89\n",
            "Step 3201. For the last 100 steps: average loss 0.6017418416999637, accuracy 84\n",
            "Step 3301. For the last 100 steps: average loss 0.484228542248709, accuracy 87\n",
            "Step 3401. For the last 100 steps: average loss 0.547054704986919, accuracy 83\n",
            "Step 3501. For the last 100 steps: average loss 0.45187883169051923, accuracy 85\n",
            "Step 3601. For the last 100 steps: average loss 0.5541910995195198, accuracy 78\n",
            "Step 3701. For the last 100 steps: average loss 0.28076148598880685, accuracy 90\n",
            "Step 3801. For the last 100 steps: average loss 0.3608453529439747, accuracy 88\n",
            "Step 3901. For the last 100 steps: average loss 0.3007601602108323, accuracy 89\n",
            "Step 4001. For the last 100 steps: average loss 0.2738797041772859, accuracy 90\n",
            "Step 4101. For the last 100 steps: average loss 0.4751247217558299, accuracy 82\n",
            "Step 4201. For the last 100 steps: average loss 0.32148553621915726, accuracy 87\n",
            "Step 4301. For the last 100 steps: average loss 0.2652322472493563, accuracy 90\n",
            "Step 4401. For the last 100 steps: average loss 0.35301571133774273, accuracy 91\n",
            "Step 4501. For the last 100 steps: average loss 0.4998649172549957, accuracy 87\n",
            "Step 4601. For the last 100 steps: average loss 0.3903797667628832, accuracy 92\n",
            "Step 4701. For the last 100 steps: average loss 0.5237267258678734, accuracy 85\n",
            "Step 4801. For the last 100 steps: average loss 0.30834674885495755, accuracy 90\n",
            "Step 4901. For the last 100 steps: average loss 0.3709264926988697, accuracy 88\n",
            "Step 5001. For the last 100 steps: average loss 0.3166357065144936, accuracy 89\n",
            "Step 5101. For the last 100 steps: average loss 0.49824566404559967, accuracy 82\n",
            "Step 5201. For the last 100 steps: average loss 0.4152513684010925, accuracy 86\n",
            "Step 5301. For the last 100 steps: average loss 0.49150382237714474, accuracy 87\n",
            "Step 5401. For the last 100 steps: average loss 0.31083593623729355, accuracy 91\n",
            "Step 5501. For the last 100 steps: average loss 0.2066941883236235, accuracy 95\n",
            "Step 5601. For the last 100 steps: average loss 0.4876235006847208, accuracy 87\n",
            "Step 5701. For the last 100 steps: average loss 0.33731469060265007, accuracy 89\n",
            "Step 5801. For the last 100 steps: average loss 0.38728605395940974, accuracy 90\n",
            "Step 5901. For the last 100 steps: average loss 0.28828263365655166, accuracy 88\n",
            "Step 6001. For the last 100 steps: average loss 0.3328475547499237, accuracy 91\n",
            "Step 6101. For the last 100 steps: average loss 0.36084976674528463, accuracy 89\n",
            "Step 6201. For the last 100 steps: average loss 0.3611426730326204, accuracy 90\n",
            "Step 6301. For the last 100 steps: average loss 0.30607847220592455, accuracy 91\n",
            "Step 6401. For the last 100 steps: average loss 0.341487128299855, accuracy 93\n",
            "Step 6501. For the last 100 steps: average loss 0.2781035387868036, accuracy 93\n",
            "Step 6601. For the last 100 steps: average loss 0.2033773658953882, accuracy 93\n",
            "Step 6701. For the last 100 steps: average loss 0.3888216923830419, accuracy 87\n",
            "Step 6801. For the last 100 steps: average loss 0.20680539662941239, accuracy 91\n",
            "Step 6901. For the last 100 steps: average loss 0.256804711178281, accuracy 91\n",
            "Step 7001. For the last 100 steps: average loss 0.40828027267873457, accuracy 87\n",
            "Step 7101. For the last 100 steps: average loss 0.37644267306870055, accuracy 87\n",
            "Step 7201. For the last 100 steps: average loss 0.44418050184399877, accuracy 86\n",
            "Step 7301. For the last 100 steps: average loss 0.4210513294829035, accuracy 86\n",
            "Step 7401. For the last 100 steps: average loss 0.40548021570437115, accuracy 92\n",
            "Step 7501. For the last 100 steps: average loss 0.22108742211519336, accuracy 93\n",
            "Step 7601. For the last 100 steps: average loss 0.3364634444934623, accuracy 91\n",
            "Step 7701. For the last 100 steps: average loss 0.2955087222956399, accuracy 91\n",
            "Step 7801. For the last 100 steps: average loss 0.345416493857575, accuracy 90\n",
            "Step 7901. For the last 100 steps: average loss 0.27297774805303904, accuracy 93\n",
            "Step 8001. For the last 100 steps: average loss 0.3697196439594233, accuracy 86\n",
            "Step 8101. For the last 100 steps: average loss 0.40396424997333596, accuracy 88\n",
            "Step 8201. For the last 100 steps: average loss 0.5096670985262818, accuracy 88\n",
            "Step 8301. For the last 100 steps: average loss 0.3311795091003369, accuracy 89\n",
            "Step 8401. For the last 100 steps: average loss 0.28390147116733055, accuracy 88\n",
            "Step 8501. For the last 100 steps: average loss 0.20496536990898298, accuracy 93\n",
            "Step 8601. For the last 100 steps: average loss 0.41444549409083004, accuracy 87\n",
            "Step 8701. For the last 100 steps: average loss 0.4380943634965347, accuracy 84\n",
            "Step 8801. For the last 100 steps: average loss 0.2406565770762446, accuracy 94\n",
            "Step 8901. For the last 100 steps: average loss 0.40034261325481657, accuracy 87\n",
            "Step 9001. For the last 100 steps: average loss 0.26113013442545446, accuracy 95\n",
            "Step 9101. For the last 100 steps: average loss 0.3456987075559654, accuracy 88\n",
            "Step 9201. For the last 100 steps: average loss 0.2610212279133933, accuracy 93\n",
            "Step 9301. For the last 100 steps: average loss 0.29588861038904535, accuracy 90\n",
            "Step 9401. For the last 100 steps: average loss 0.44011612700225344, accuracy 86\n",
            "Step 9501. For the last 100 steps: average loss 0.360423608547807, accuracy 90\n",
            "Step 9601. For the last 100 steps: average loss 0.2596698002434959, accuracy 93\n",
            "Step 9701. For the last 100 steps: average loss 0.6317176986114973, accuracy 82\n",
            "Step 9801. For the last 100 steps: average loss 0.4439453982368247, accuracy 90\n",
            "Step 9901. For the last 100 steps: average loss 0.34554978934225916, accuracy 88\n",
            "CPU times: user 1min 46s, sys: 436 ms, total: 1min 46s\n",
            "Wall time: 1min 47s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "train(network, X_train, y_train, epochs=1, learning_rate=0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnIg4t1Tle7n"
      },
      "source": [
        "# 4/ Tính độ chính xác của mạng CNN khi dự đoán cho tập test 10000 ảnh khác với tập train do tensorflow cung cấp:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "dQ-lLypJXFLX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "684ce507-2e70-4508-f4c0-612a02205eba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for the test set is 90.07\n"
          ]
        }
      ],
      "source": [
        "evaluate(network, X_test, y_test)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}