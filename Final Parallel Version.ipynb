{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Parallel CNN layers for image classification"
      ],
      "metadata": {
        "id": "4LySzmcIPMmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Group members"
      ],
      "metadata": {
        "id": "jMPsri9uQdDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Member | Student ID |\n",
        "|--------|------------|\n",
        "| Nguyễn Thế Hưng | 19127154 |\n",
        "| Trần Minh Thiện | 19127281 |\n",
        "| Lê Tâm Anh | 19127330 |"
      ],
      "metadata": {
        "id": "pSlR_LyOPhmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numba import config\n",
        "from numba import jit, cuda, prange\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import time"
      ],
      "metadata": {
        "id": "4wVQdkeSPNiU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYTM62eTY1af"
      },
      "source": [
        "# Functions for Parallell Convolutional Layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rooLF511owYL"
      },
      "outputs": [],
      "source": [
        "@cuda.jit\n",
        "def cnn_forward_kernel(patches, kernels, convolution_output):\n",
        "    r, c = cuda.grid(2)\n",
        "\n",
        "    if r < patches.shape[0] and c < patches.shape[1]:\n",
        "        for k in range(kernels.shape[0]):\n",
        "            sum = 0\n",
        "            for i in range(kernels.shape[1]):\n",
        "                for j in range(kernels.shape[2]):\n",
        "                    sum += patches[r, c, i, j] * kernels[k, i, j]\n",
        "            convolution_output[r, c, k] = sum\n",
        "\n",
        "@cuda.jit\n",
        "def cnn_backward_kernel(patches, dE_dY, dE_dk):\n",
        "    x, y, z = cuda.grid(3)\n",
        "\n",
        "    if x < dE_dk.shape[0] and y < dE_dk.shape[1] and z < dE_dk.shape[2]:\n",
        "        temp = 0\n",
        "        for h in range(patches.shape[0]):\n",
        "            for w in range(patches.shape[1]):\n",
        "                temp += patches[h, w, y, z] * dE_dY[h, w, x]\n",
        "        dE_dk[x, y, z] = temp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K05mzy-EZTZv"
      },
      "source": [
        "# Functions for Parallell Max Pooling Layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2mLxP8vGcrJk"
      },
      "outputs": [],
      "source": [
        "#@cuda.jit(device=True)\n",
        "def patches_generator(image,kernel_size):\n",
        "        \"\"\"\n",
        "        Divide the input image in patches to be used during pooling.\n",
        "        Yields the tuples containing the patches and their coordinates.\n",
        "        \"\"\"\n",
        "        # Compute the ouput size\n",
        "        output_h = image.shape[0] // kernel_size\n",
        "        output_w = image.shape[1] // kernel_size\n",
        "        #self.image = image\n",
        "        #c,r=cuda.grid(2)\n",
        "        for h in range(output_h):\n",
        "            for w in range(output_w):\n",
        "                patch = image[(h*kernel_size):(h*kernel_size+kernel_size), (w*kernel_size):(w*kernel_size+kernel_size)]\n",
        "                yield patch,h,w\n",
        "\n",
        "def forward_prop(image,kernel_size):\n",
        "        image_h, image_w, num_kernels = image.shape\n",
        "        max_pooling_output = np.zeros((image_h//kernel_size, image_w//kernel_size, num_kernels))\n",
        "        for patch, h, w in patches_generator(image,kernel_size):\n",
        "            max_pooling_output[h,w] = np.amax(patch, axis=(0,1))\n",
        "        return max_pooling_output\n",
        "\n",
        "\n",
        "\n",
        "def back_prop(image,dE_dY,kernel_size):\n",
        "        \"\"\"\n",
        "        Takes the gradient of the loss function with respect to the output and computes the gradients of the loss function with respect\n",
        "        to the kernels' weights.\n",
        "        dE_dY comes from the following layer, typically softmax.\n",
        "        There are no weights to update, but the output is needed to update the weights of the convolutional layer.\n",
        "        \"\"\"\n",
        "        dE_dk_temp1 = np.zeros(image.shape)\n",
        "        #dE_dk=np.ascontiguousarray(dE_dk_temp1)\n",
        "        #cuda.pinned(dE_dk)\n",
        "        for patch,h,w in patches_generator(image,kernel_size):\n",
        "            image_h, image_w, num_kernels = patch.shape\n",
        "            max_val = np.amax(patch, axis=(0,1))\n",
        "            block_size = (16, 16)\n",
        "            grid_size = (math.ceil(image_w / block_size[0]),\n",
        "                        math.ceil(image_h / block_size[1]))\n",
        "            patch=np.ascontiguousarray(patch)\n",
        "\n",
        "            dA=cuda.to_device(patch)\n",
        "            dE=cuda.to_device(dE_dk_temp1)\n",
        "            back_prob_sup[grid_size, block_size](image_h,image_w,num_kernels,dA,max_val,dE_dY,dE,h,w,kernel_size)\n",
        "            dE_dk=dE.copy_to_host()\n",
        "            return dE_dk\n",
        "\n",
        "@cuda.jit\n",
        "def back_prob_sup(image_h,image_w,num_kernels,patch,max_val,dE_dY,dE_dk,h,w,kernel_size):\n",
        "    c,r=cuda.grid(2)\n",
        "    #print(\"hihi\")\n",
        "    if r < image_h and c < image_w:\n",
        "          for idx_k in range(num_kernels):\n",
        "                if patch[r,c,idx_k] == max_val[idx_k]:\n",
        "                  #print(\"hihi\")\n",
        "                  dE_dk[h*kernel_size+r, w*kernel_size+c, idx_k] = dE_dY[h,w,idx_k]\n",
        "    #return dE_dk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMRZrx-iwVvA"
      },
      "source": [
        "# Functions for Parallell Softmax Layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LOVIFj25zufr"
      },
      "outputs": [],
      "source": [
        "#Hàm nhân vector với ma trận\n",
        "@cuda.jit\n",
        "def dot(a, b, c):\n",
        "  col = cuda.grid(1)\n",
        "  if (col < b.shape[1]):\n",
        "    sum = 0.0\n",
        "    for i in range(b.shape[0]):\n",
        "      sum += a[i] * b[i, col]\n",
        "    c[col] = sum\n",
        "\n",
        "#Hàm nhân ma trận với vector\n",
        "@cuda.jit\n",
        "def cu_matrix_vector(A, b, c):\n",
        "  row = cuda.grid(1)\n",
        "  if (row < A.shape[0]):\n",
        "    sum = 0.0\n",
        "    for i in range(A.shape[1]):\n",
        "      sum += A[row, i] * b[i]\n",
        "    c[row] = sum\n",
        "\n",
        "#Hàm nhân hai ma trận\n",
        "@cuda.jit\n",
        "def matmul(A,B,C):\n",
        "  i,j = cuda.grid(2)\n",
        "  if i < C.shape[0] and j < C.shape[1]:\n",
        "    tmp = A[i,0] * B[0,j]\n",
        "    C[i,j] = tmp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_TlPrapxASM"
      },
      "source": [
        "# CNN classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5VK4gXE3w-Pc"
      },
      "outputs": [],
      "source": [
        "class ConvolutionLayer:\n",
        "    def __init__(self, kernel_num, kernel_size):\n",
        "        \"\"\"\n",
        "        Constructor takes as input the number of kernels and their size. I assume only squared filters of size kernel_size x kernel_size\n",
        "        \"\"\"\n",
        "        self.kernel_num = kernel_num\n",
        "        self.kernel_size = kernel_size\n",
        "        # Generate random filters of shape (kernel_num, kernel_size, kernel_size). Divide by kernel_size^2 for weight normalization\n",
        "        self.kernels = np.random.randn(kernel_num, kernel_size, kernel_size) / (kernel_size**2)\n",
        "\n",
        "    def patches_generator(self, image):\n",
        "        \"\"\"\n",
        "        Divide the input image in patches to be used during convolution.\n",
        "        Yields the tuples containing the patches and their coordinates.\n",
        "        \"\"\"\n",
        "        # Extract image height and width\n",
        "        image_h, image_w = image.shape\n",
        "        self.image = image\n",
        "        # The number of patches, given a fxf filter is h-f+1 for height and w-f+1 for width\n",
        "        patches = np.empty((image_h-self.kernel_size+1, image_w-self.kernel_size+1, self.kernel_size, self.kernel_size))\n",
        "        for h in range(image_h-self.kernel_size+1):\n",
        "            for w in range(image_w-self.kernel_size+1):\n",
        "                patches[h, w] = image[h:(h+self.kernel_size), w:(w+self.kernel_size)]\n",
        "        return patches\n",
        "\n",
        "    def forward_prop(self, image):\n",
        "        \"\"\"\n",
        "        Perform forward propagation for the convolutional layer.\n",
        "        \"\"\"\n",
        "        # Extract image height and width\n",
        "        image_h, image_w = image.shape\n",
        "        # Initialize the convolution output volume of the correct size\n",
        "        convolution_output = np.zeros((image_h-self.kernel_size+1, image_w-self.kernel_size+1, self.kernel_num))\n",
        "        # Unpack the generator\n",
        "        patches = self.patches_generator(image)\n",
        "        block_size = (16, 16)\n",
        "        grid_size = (math.ceil(convolution_output.shape[1] / block_size[0]),\n",
        "                     math.ceil(convolution_output.shape[0] / block_size[1]))\n",
        "        cnn_forward_kernel[grid_size, block_size](patches, self.kernels, convolution_output)\n",
        "        cuda.synchronize()\n",
        "        return convolution_output\n",
        "\n",
        "    def back_prop(self, dE_dY, alpha):\n",
        "        \"\"\"\n",
        "        Takes the gradient of the loss function with respect to the output and computes the gradients of the loss function with respect\n",
        "        to the kernels' weights.\n",
        "        dE_dY comes from the following layer, typically max pooling layer.\n",
        "        It updates the kernels' weights\n",
        "        \"\"\"\n",
        "        # Initialize gradient of the loss function with respect to the kernel weights\n",
        "        dE_dk = np.zeros(self.kernels.shape)\n",
        "        patches = self.patches_generator(self.image)\n",
        "        block_size = (16, 4, 4)\n",
        "        grid_size = (math.ceil(dE_dk.shape[2] / block_size[0]),\n",
        "                     math.ceil(dE_dk.shape[1] / block_size[1]),\n",
        "                     math.ceil(dE_dk.shape[0] / block_size[2]))\n",
        "        cnn_backward_kernel[grid_size, block_size](patches, dE_dY, dE_dk)\n",
        "        cuda.synchronize()\n",
        "        # Update the parameters\n",
        "        self.kernels -= alpha*dE_dk\n",
        "        return dE_dk\n",
        "\n",
        "\n",
        "class MaxPoolingLayer:\n",
        "    def __init__(self, kernel_size):\n",
        "        \"\"\"\n",
        "        Constructor takes as input the size of the kernel\n",
        "        \"\"\"\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def patches_generator(self, image):\n",
        "        \"\"\"\n",
        "        Divide the input image in patches to be used during pooling.\n",
        "        Yields the tuples containing the patches and their coordinates.\n",
        "        \"\"\"\n",
        "        # Compute the ouput size\n",
        "        output_h = image.shape[0] // self.kernel_size\n",
        "        output_w = image.shape[1] // self.kernel_size\n",
        "        self.image = image\n",
        "\n",
        "        for h in range(output_h):\n",
        "            for w in range(output_w):\n",
        "                patch = image[(h*self.kernel_size):(h*self.kernel_size+self.kernel_size), (w*self.kernel_size):(w*self.kernel_size+self.kernel_size)]\n",
        "                yield patch, h, w\n",
        "\n",
        "    def forward_prop(self, image):\n",
        "        image_h, image_w, num_kernels = image.shape\n",
        "        max_pooling_output = np.zeros((image_h//self.kernel_size, image_w//self.kernel_size, num_kernels))\n",
        "        for patch, h, w in self.patches_generator(image):\n",
        "            max_pooling_output[h,w] = np.amax(patch, axis=(0,1)) # Chia nhỏ để tìm số lớn nhất song song sau đó so sánh với nhau\n",
        "        return max_pooling_output\n",
        "\n",
        "    def back_prop(self, dE_dY):\n",
        "        \"\"\"\n",
        "        Takes the gradient of the loss function with respect to the output and computes the gradients of the loss function with respect\n",
        "        to the kernels' weights.\n",
        "        dE_dY comes from the following layer, typically softmax.\n",
        "        There are no weights to update, but the output is needed to update the weights of the convolutional layer.\n",
        "        \"\"\"\n",
        "        dE_dk = np.zeros(self.image.shape)\n",
        "        for patch,h,w in self.patches_generator(self.image):\n",
        "            image_h, image_w, num_kernels = patch.shape\n",
        "            max_val = np.amax(patch, axis=(0,1))\n",
        "\n",
        "            for idx_h in range(image_h):\n",
        "                for idx_w in range(image_w):\n",
        "                    for idx_k in range(num_kernels):\n",
        "                        if patch[idx_h,idx_w,idx_k] == max_val[idx_k]:\n",
        "                            dE_dk[h*self.kernel_size+idx_h, w*self.kernel_size+idx_w, idx_k] = dE_dY[h,w,idx_k]\n",
        "            return dE_dk\n",
        "\n",
        "\n",
        "class SoftmaxLayer:\n",
        "    \"\"\"\n",
        "    Takes the volume coming from convolutional & pooling layers. It flattens it and it uses it in the next layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_units, output_units):\n",
        "        # Initiallize weights and biases\n",
        "        self.weight = np.random.randn(input_units, output_units)/input_units\n",
        "        self.bias = np.zeros(output_units)\n",
        "\n",
        "    def forward_prop(self, image):\n",
        "      self.original_shape = image.shape # stored for backprop\n",
        "      # Flatten the image\n",
        "      #print(\"image: \", image)\n",
        "      image_flattened = image.flatten()\n",
        "      #print(\"image_flattened: \", image_flattened)\n",
        "      self.flattened_input = image_flattened # stored for backprop\n",
        "\n",
        "      # Perform matrix multiplication and add bias\n",
        "      C = np.empty(10)\n",
        "      dA = cuda.to_device(image_flattened)\n",
        "      dB = cuda.to_device(self.weight)\n",
        "      dC = cuda.to_device(C)\n",
        "      dot[(self.weight.shape[0]+255)//256, 256](dA,dB,dC)\n",
        "      #cu_matrix_vector[(dZ_dX.shape[0]+511)//512, 512](dZ_dX,dE_dZ,C)\n",
        "      result = dC.copy_to_host()\n",
        "      first_output = result  + self.bias\n",
        "      self.output = first_output\n",
        "      # Apply softmax activation\n",
        "      softmax_output = np.exp(first_output) / np.sum(np.exp(first_output), axis=0)\n",
        "\n",
        "      return softmax_output\n",
        "\n",
        "\n",
        "    def back_prop(self, dE_dY, alpha):\n",
        "      for i, gradient in enumerate(dE_dY):\n",
        "        if gradient == 0:\n",
        "          continue\n",
        "        transformation_eq = np.exp(self.output)\n",
        "        S_total = np.sum(transformation_eq)\n",
        "\n",
        "        # Compute gradients with respect to output (Z)\n",
        "        dY_dZ = -transformation_eq[i]*transformation_eq / (S_total**2)\n",
        "        dY_dZ[i] = transformation_eq[i]*(S_total - transformation_eq[i]) / (S_total**2)\n",
        "\n",
        "        # Compute gradients of output Z with respect to weight, bias, input\n",
        "        dZ_dw = self.flattened_input\n",
        "        dZ_db = 1\n",
        "        dZ_dX = self.weight\n",
        "\n",
        "        # Gradient of loss with respect ot output\n",
        "        dE_dZ = gradient * dY_dZ\n",
        "\n",
        "        # Gradient of loss with respect to weight, bias, input\n",
        "\n",
        "        #C = np.empty((dZ_dw[np.newaxis].T.shape[0], dE_dZ[np.newaxis].shape[1]))\n",
        "        #dA = cuda.to_device(dZ_dw[np.newaxis].T)\n",
        "        #dB = cuda.to_device(dE_dZ[np.newaxis])\n",
        "        #dC = cuda.to_device(C)\n",
        "        #blockx = int(np.ceil(C.shape[0] / 16))\n",
        "        #blocky = int(np.ceil(C.shape[1] / 16))\n",
        "        #blockspergrid = (blockx, blocky)\n",
        "        #matmul[blockspergrid, (16,16)](dA,dB,dC)\n",
        "        #dE_dW = dC.copy_to_host()\n",
        "        dE_dW = dZ_dw[np.newaxis].T @ dE_dZ[np.newaxis]\n",
        "\n",
        "        # Matrix-vector multiply function\n",
        "        C = np.empty(dZ_dX.shape[0])\n",
        "        dA = cuda.to_device(dZ_dX)\n",
        "        dB = cuda.to_device(dE_dZ)\n",
        "        dC = cuda.to_device(C)\n",
        "        cu_matrix_vector[(dZ_dX.shape[0]+15)//16, 16](dA,dB,dC)\n",
        "        dE_dX = dC.copy_to_host()\n",
        "\n",
        "       # dE_dX = dZ_dX @ dE_dZ\n",
        "\n",
        "        # Update parameters\n",
        "        self.weight -= alpha * dE_dW\n",
        "        self.bias -= alpha * (dE_dZ * dZ_db)\n",
        "\n",
        "        return dE_dX.reshape(self.original_shape)\n",
        "\n",
        "def CNN_forward(image, label, layers):\n",
        "    output = image/255.\n",
        "    for layer in layers:\n",
        "        output = layer.forward_prop(output)\n",
        "    # Compute loss (cross-entropy) and accuracy\n",
        "    loss = -np.log(output[label])\n",
        "    accuracy = 1 if np.argmax(output) == label else 0\n",
        "    return output, loss, accuracy\n",
        "\n",
        "def CNN_backprop(gradient, layers, alpha=0.05):\n",
        "    grad_back = gradient\n",
        "    for layer in layers[::-1]:\n",
        "        if type(layer) in [ConvolutionLayer, SoftmaxLayer]:\n",
        "            grad_back = layer.back_prop(grad_back, alpha)\n",
        "        elif type(layer) == MaxPoolingLayer:\n",
        "            grad_back = layer.back_prop(grad_back)\n",
        "    return grad_back\n",
        "\n",
        "\n",
        "def CNN_training(image, label, layers, alpha=0.05):\n",
        "    # Forward step\n",
        "    output, loss, accuracy = CNN_forward(image, label, layers)\n",
        "\n",
        "    # Initial gradient\n",
        "    gradient = np.zeros(10)\n",
        "    gradient[label] = -1/output[label]\n",
        "\n",
        "    # Backprop step\n",
        "    gradient_back = CNN_backprop(gradient, layers, alpha)\n",
        "\n",
        "    return loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWYqMGGUZjGg"
      },
      "source": [
        "# Main function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YKXC0Rljm2j"
      },
      "source": [
        "## Train function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5_mA6IhAQrEF"
      },
      "outputs": [],
      "source": [
        "def train(network, X_train, y_train, epochs = 1, learning_rate = 0.05, verbose = True):\n",
        "    for epoch in range(1):\n",
        "        if verbose == True:\n",
        "            print('Epoch {} ->'.format(epoch+1))\n",
        "        # Shuffle training data\n",
        "        permutation = np.random.permutation(len(X_train))\n",
        "        X_train = X_train[permutation]\n",
        "        y_train = y_train[permutation]\n",
        "        # Training the CNN\n",
        "        loss = 0\n",
        "        accuracy = 0\n",
        "        for i, (image, label) in enumerate(zip(X_train, y_train)):\n",
        "            if i % 100 == 0: # Every 100 examples\n",
        "                if verbose == True:\n",
        "                    print(\"Step {}. For the last 100 steps: average loss {}, accuracy {}\".format(i+1, loss/100, accuracy))\n",
        "                loss = 0\n",
        "                accuracy = 0\n",
        "            loss_1, accuracy_1 = CNN_training(image, label, network)\n",
        "            loss += loss_1\n",
        "            accuracy += accuracy_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa_FDwW3jslW"
      },
      "source": [
        "## Predict & Evaluate functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Lta9bV4iQx32"
      },
      "outputs": [],
      "source": [
        "def predict(network, image):\n",
        "    output = image/255.\n",
        "    for layer in network:\n",
        "        output = layer.forward_prop(output)\n",
        "    return np.argmax(output) # return a number\n",
        "def evaluate(network, X_test, y_test):\n",
        "    correct = 0\n",
        "    for x, y in zip(X_test, y_test):\n",
        "        pred = predict(network, x)\n",
        "        if y == pred:\n",
        "            correct += 1\n",
        "    acc = correct / y_test.shape[0]\n",
        "    print(f'Accuracy for the test set is {acc *100}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOOaJp_Oj3wa"
      },
      "source": [
        "## Loading train data and define network layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0OaYdNsHWnjZ"
      },
      "outputs": [],
      "source": [
        "network = [\n",
        "    ConvolutionLayer(16,3), # layer with 8 3x3 filters, output (26,26,16)\n",
        "    MaxPoolingLayer(2), # pooling layer 2x2, output (13,13,16)\n",
        "    SoftmaxLayer(13*13*16, 10) # softmax layer with 13*13*16 input and 10 output\n",
        "    ]\n",
        "\n",
        "# Load training data\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train[:10000]\n",
        "y_train = y_train[:10000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wolseb1OkAqH"
      },
      "source": [
        "## Training CNN and calculating running time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N8p_68OQ-CX",
        "outputId": "644b58c4-fe98-4c4f-8c68-ad199fd8c40c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 ->\n",
            "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 4 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:885: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 11 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 4 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:885: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 101. For the last 100 steps: average loss 1.9097160814767398, accuracy 35\n",
            "Step 201. For the last 100 steps: average loss 1.109052480010852, accuracy 67\n",
            "Step 301. For the last 100 steps: average loss 0.8324638541247977, accuracy 76\n",
            "Step 401. For the last 100 steps: average loss 0.7470558742310915, accuracy 79\n",
            "Step 501. For the last 100 steps: average loss 0.6985758651196469, accuracy 79\n",
            "Step 601. For the last 100 steps: average loss 0.7519752213159417, accuracy 78\n",
            "Step 701. For the last 100 steps: average loss 0.680425700848134, accuracy 78\n",
            "Step 801. For the last 100 steps: average loss 0.475219389278869, accuracy 86\n",
            "Step 901. For the last 100 steps: average loss 0.606489227591856, accuracy 82\n",
            "Step 1001. For the last 100 steps: average loss 0.4948608858440089, accuracy 84\n",
            "Step 1101. For the last 100 steps: average loss 0.35734936609070594, accuracy 93\n",
            "Step 1201. For the last 100 steps: average loss 0.4959704418301374, accuracy 83\n",
            "Step 1301. For the last 100 steps: average loss 0.48724983927212245, accuracy 88\n",
            "Step 1401. For the last 100 steps: average loss 0.48778832167330405, accuracy 87\n",
            "Step 1501. For the last 100 steps: average loss 0.4445653434608705, accuracy 86\n",
            "Step 1601. For the last 100 steps: average loss 0.41488796553184387, accuracy 86\n",
            "Step 1701. For the last 100 steps: average loss 0.4393196911401791, accuracy 88\n",
            "Step 1801. For the last 100 steps: average loss 0.5930984955502263, accuracy 81\n",
            "Step 1901. For the last 100 steps: average loss 0.4484795848683505, accuracy 87\n",
            "Step 2001. For the last 100 steps: average loss 0.42314807355556444, accuracy 89\n",
            "Step 2101. For the last 100 steps: average loss 0.39591802960397876, accuracy 88\n",
            "Step 2201. For the last 100 steps: average loss 0.3394099448965477, accuracy 89\n",
            "Step 2301. For the last 100 steps: average loss 0.29917365418477093, accuracy 90\n",
            "Step 2401. For the last 100 steps: average loss 0.4793479963460536, accuracy 86\n",
            "Step 2501. For the last 100 steps: average loss 0.4729576456826836, accuracy 87\n",
            "Step 2601. For the last 100 steps: average loss 0.22860799891043967, accuracy 94\n",
            "Step 2701. For the last 100 steps: average loss 0.5652421649824027, accuracy 88\n",
            "Step 2801. For the last 100 steps: average loss 0.38464239495453834, accuracy 90\n",
            "Step 2901. For the last 100 steps: average loss 0.32368111268580213, accuracy 91\n",
            "Step 3001. For the last 100 steps: average loss 0.35747090420898864, accuracy 87\n",
            "Step 3101. For the last 100 steps: average loss 0.44303025395284223, accuracy 90\n",
            "Step 3201. For the last 100 steps: average loss 0.5013051860685532, accuracy 85\n",
            "Step 3301. For the last 100 steps: average loss 0.4929517235137619, accuracy 87\n",
            "Step 3401. For the last 100 steps: average loss 0.26240845397069046, accuracy 92\n",
            "Step 3501. For the last 100 steps: average loss 0.4149246848722698, accuracy 93\n",
            "Step 3601. For the last 100 steps: average loss 0.4148297120501473, accuracy 86\n",
            "Step 3701. For the last 100 steps: average loss 0.41482951161199083, accuracy 88\n",
            "Step 3801. For the last 100 steps: average loss 0.29370417000717675, accuracy 92\n",
            "Step 3901. For the last 100 steps: average loss 0.4241334107133947, accuracy 89\n",
            "Step 4001. For the last 100 steps: average loss 0.23908834000321974, accuracy 94\n",
            "Step 4101. For the last 100 steps: average loss 0.3159466786308513, accuracy 89\n",
            "Step 4201. For the last 100 steps: average loss 0.3564486046494529, accuracy 88\n",
            "Step 4301. For the last 100 steps: average loss 0.3160447229635251, accuracy 92\n",
            "Step 4401. For the last 100 steps: average loss 0.40139469677471445, accuracy 86\n",
            "Step 4501. For the last 100 steps: average loss 0.29465485936951774, accuracy 92\n",
            "Step 4601. For the last 100 steps: average loss 0.48196297268406496, accuracy 87\n",
            "Step 4701. For the last 100 steps: average loss 0.30076041612826765, accuracy 91\n",
            "Step 4801. For the last 100 steps: average loss 0.3398304957566701, accuracy 91\n",
            "Step 4901. For the last 100 steps: average loss 0.2914944248562734, accuracy 95\n",
            "Step 5001. For the last 100 steps: average loss 0.21815364422543046, accuracy 93\n",
            "Step 5101. For the last 100 steps: average loss 0.28177752520872784, accuracy 91\n",
            "Step 5201. For the last 100 steps: average loss 0.22619199749382574, accuracy 92\n",
            "Step 5301. For the last 100 steps: average loss 0.45023021075465713, accuracy 85\n",
            "Step 5401. For the last 100 steps: average loss 0.44978807093307904, accuracy 89\n",
            "Step 5501. For the last 100 steps: average loss 0.3300577337767679, accuracy 90\n",
            "Step 5601. For the last 100 steps: average loss 0.29214744211565974, accuracy 89\n",
            "Step 5701. For the last 100 steps: average loss 0.3070932132337646, accuracy 91\n",
            "Step 5801. For the last 100 steps: average loss 0.2151610657958296, accuracy 94\n",
            "Step 5901. For the last 100 steps: average loss 0.4452476565954146, accuracy 91\n",
            "Step 6001. For the last 100 steps: average loss 0.30247720454177995, accuracy 93\n",
            "Step 6101. For the last 100 steps: average loss 0.33877392421191244, accuracy 92\n",
            "Step 6201. For the last 100 steps: average loss 0.2513809107152192, accuracy 94\n",
            "Step 6301. For the last 100 steps: average loss 0.21455842805420736, accuracy 92\n",
            "Step 6401. For the last 100 steps: average loss 0.35421350971090343, accuracy 91\n",
            "Step 6501. For the last 100 steps: average loss 0.2769964360207378, accuracy 93\n",
            "Step 6601. For the last 100 steps: average loss 0.2607094913563004, accuracy 93\n",
            "Step 6701. For the last 100 steps: average loss 0.2180773752301339, accuracy 95\n",
            "Step 6801. For the last 100 steps: average loss 0.21700677708746613, accuracy 96\n",
            "Step 6901. For the last 100 steps: average loss 0.2762460362118735, accuracy 90\n",
            "Step 7001. For the last 100 steps: average loss 0.3070745867355016, accuracy 91\n",
            "Step 7101. For the last 100 steps: average loss 0.488187932752557, accuracy 86\n",
            "Step 7201. For the last 100 steps: average loss 0.3508873114524453, accuracy 90\n",
            "Step 7301. For the last 100 steps: average loss 0.3788702040493832, accuracy 86\n",
            "Step 7401. For the last 100 steps: average loss 0.5237493645676744, accuracy 88\n",
            "Step 7501. For the last 100 steps: average loss 0.20475367870326447, accuracy 93\n",
            "Step 7601. For the last 100 steps: average loss 0.2807221531229965, accuracy 89\n",
            "Step 7701. For the last 100 steps: average loss 0.25686815947559144, accuracy 91\n",
            "Step 7801. For the last 100 steps: average loss 0.2400537938983832, accuracy 93\n",
            "Step 7901. For the last 100 steps: average loss 0.3383043610194412, accuracy 91\n",
            "Step 8001. For the last 100 steps: average loss 0.4965321091105765, accuracy 86\n",
            "Step 8101. For the last 100 steps: average loss 0.3527738543117369, accuracy 89\n",
            "Step 8201. For the last 100 steps: average loss 0.3177842765223049, accuracy 88\n",
            "Step 8301. For the last 100 steps: average loss 0.3017483813494482, accuracy 93\n",
            "Step 8401. For the last 100 steps: average loss 0.36599463887344036, accuracy 88\n",
            "Step 8501. For the last 100 steps: average loss 0.5804057682839667, accuracy 89\n",
            "Step 8601. For the last 100 steps: average loss 0.38555559651604093, accuracy 91\n",
            "Step 8701. For the last 100 steps: average loss 0.22281887340932152, accuracy 93\n",
            "Step 8801. For the last 100 steps: average loss 0.3537973098586234, accuracy 91\n",
            "Step 8901. For the last 100 steps: average loss 0.23967511625092314, accuracy 94\n",
            "Step 9001. For the last 100 steps: average loss 0.3049757330784011, accuracy 93\n",
            "Step 9101. For the last 100 steps: average loss 0.2983938352270855, accuracy 89\n",
            "Step 9201. For the last 100 steps: average loss 0.39681902598967406, accuracy 86\n",
            "Step 9301. For the last 100 steps: average loss 0.34382658897883694, accuracy 90\n",
            "Step 9401. For the last 100 steps: average loss 0.1289886218215358, accuracy 97\n",
            "Step 9501. For the last 100 steps: average loss 0.29613921777848984, accuracy 91\n",
            "Step 9601. For the last 100 steps: average loss 0.28604670594649556, accuracy 92\n",
            "Step 9701. For the last 100 steps: average loss 0.2462524952622631, accuracy 93\n",
            "Step 9801. For the last 100 steps: average loss 0.2926099719654286, accuracy 92\n",
            "Step 9901. For the last 100 steps: average loss 0.3029162589502648, accuracy 90\n",
            "CPU times: user 1min 28s, sys: 1.13 s, total: 1min 29s\n",
            "Wall time: 1min 29s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "train(network, X_train, y_train, epochs=1, learning_rate=0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnIg4t1Tle7n"
      },
      "source": [
        "## Predict for 10000 images and find the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dQ-lLypJXFLX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10388fba-e52a-4015-a165-67111f67b246"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for the test set is 91.47\n"
          ]
        }
      ],
      "source": [
        "evaluate(network, X_test, y_test)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}